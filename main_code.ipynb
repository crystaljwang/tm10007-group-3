{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystaljwang/tm10007_group_3/blob/main/main_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kWuBcw6X2POx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All necessary libraries\n",
        "All libraries used in this code are run here"
      ],
      "metadata": {
        "id": "JrS8Edsz2RKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n39THIh1idwq"
      },
      "outputs": [],
      "source": [
        "# ----- Import necessary libraries -----\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from scipy.stats import shapiro \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LExRxWGVRAyR"
      },
      "source": [
        "# Imports\n",
        "\n",
        "The code below loads the GIST data from GitHub. This was used once to create the test and train set. It is not needed to run this again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUQ41FV4Q7m9",
        "outputId": "1a99f0f3-dc1d-4e75-b8a2-837d965ad9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: git+https://github.com/crystaljwang/tm10007_group_3.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# # Run this to use from colab environment\n",
        "# !pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git\n",
        "\n",
        "# # Run this to use from colab environment\n",
        "# !git clone https://github.com/jveenland/tm10007_ml.git\n",
        "\n",
        "# %cd /content/tm10007_ml/worcgist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOwJctD-Ri7N",
        "outputId": "d9bff898-935b-4e08-ab19-08dd6bbafa47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of samples: 246\n",
            "The number of columns: 494\n"
          ]
        }
      ],
      "source": [
        "# # ---- Import data -----\n",
        "# dir = Path('.') / 'GIST_radiomicFeatures.csv'\n",
        "# data = pd.read_csv(dir, index_col=0)\n",
        "\n",
        "# print(f'The number of samples: {len(data.index)}')\n",
        "# print(f'The number of columns: {len(data.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the data\n",
        "This code was used to split the data the first time. To make sure we were all using the same train and test data we wrote csv files for the train and test data, that we are loading again below. This code is used once, and is not necessary to run again."
      ],
      "metadata": {
        "id": "LosnsE8Zj3Dq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mjZvZNlpidwr",
        "outputId": "f185d65d-89ca-44ae-9077-5df26255a9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tm10007_group_3' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: '/content/tm10007_group_3'\n",
            "/content/tm10007_ml/worcgist\n",
            "/content/tm10007_ml/worcgist\n"
          ]
        }
      ],
      "source": [
        "# # Replace label values from string to binary\n",
        "# data['label'] = data['label'].replace({'GIST': 1, 'non-GIST': 0})\n",
        "\n",
        "# # Separate the features and labels\n",
        "# X = data.drop(['label'], axis=1)\n",
        "# y = data['label']\n",
        "\n",
        "# # Split the data into random train and test sets\n",
        "# X_train_tot, X_test, y_train_tot, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Write the X_test DataFrame to a CSV file\n",
        "# X_train_tot.to_csv('X_train_tot.csv', index=False)\n",
        "# X_test.to_csv('X_test.csv', index=False)\n",
        "# y_train_tot.to_csv('y_train_tot.csv', index=False)\n",
        "# y_test.to_csv('y_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the train data"
      ],
      "metadata": {
        "id": "zxTK4sC_ocls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load split data from our repository\n",
        "url = \"https://raw.githubusercontent.com/crystaljwang/tm10007_group_3/main/data/\"\n",
        "X_train_tot = pd.read_csv(url + 'X_train_tot.csv')\n",
        "y_train_tot = pd.read_csv(url + 'y_train_tot.csv')\n",
        "X_test = pd.read_csv(url + 'X_test.csv')\n",
        "y_test = pd.read_csv(url + 'y_test.csv')\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_tot, y_train_tot, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "qWDAzvu1oZ0X"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the data"
      ],
      "metadata": {
        "id": "HFXEtt6Rj8P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage GIST vs Non-GIST\n",
        "counts = y_train.value_counts(normalize=True)\n",
        "percentage_nongist = counts[0] * 100\n",
        "percentage_gist = counts[1] * 100\n",
        "\n",
        "print(f'Percentage of non-GIST in training set: {percentage_nongist:.2f}%')\n",
        "print(f'Percentage of GIST in training set: {percentage_gist:.2f}%')\n",
        "\n",
        "# Check for missing data\n",
        "if X_train.isnull().sum().sum() > 0:\n",
        "    print('Missing data found.')\n",
        "    exit()\n",
        "else:\n",
        "    print('No missing data found.')\n",
        "\n",
        "# Check for categorial values\n",
        "categorical_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f'Categorical columns found: {categorical_cols}')\n",
        "    exit()\n",
        "else:\n",
        "    print('No categorical columns found.')\n",
        "\n",
        "# Perform Shapiro-Wilk test for normality\n",
        "p_values_above_threshold = []\n",
        "for col in X_train.columns:\n",
        "  stat, p = shapiro((X_train))\n",
        "  p_values_above_threshold.append(p > 0.05)\n",
        "  #print(f'Shapiro test for column {col}: statistic = {stat:.3f}, p-value = {p:.3f}')\n",
        "\n",
        "percent_above_threshold = sum(p_values_above_threshold) / len(p_values_above_threshold) * 100\n",
        "print(f'{percent_above_threshold:.1f} percent of the data is normally distributed.')\n",
        "\n",
        "#print('stat=%.3f, p=%.3f\\n' % (stat, p))"
      ],
      "metadata": {
        "id": "wrp508pykLGR",
        "outputId": "2451e061-7c81-436a-a5b9-0961f8a379cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of non-GIST in training set: 51.20%\n",
            "Percentage of GIST in training set: 48.80%\n",
            "No missing data found.\n",
            "No categorical columns found.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/scipy/stats/_morestats.py:1816: UserWarning: p-value may not be accurate for N > 5000.\n",
            "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 percent of the data is normally distributed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "4U0h-PMmj940"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "x9mCc9bUidwr"
      },
      "outputs": [],
      "source": [
        "# ----- Outliers -----\n",
        "\n",
        "def replace_outliers(data):\n",
        "    \"\"\"\n",
        "    Replaces the outliers in a DataFrame with the lower or upper bound.\n",
        "\n",
        "    :param data: The DataFrame to be filtered\n",
        "    :return: A new DataFrame with the outliers replaced by the lower or upper bound for each column\n",
        "    \"\"\"\n",
        "    # Calculate the lower and upper bounds based on each column's median and interquartile range\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Replace the outliers with the lower or upper bound\n",
        "    for col in data.columns:\n",
        "        data[col] = data[col].apply(lambda x: upper_bound[col] if x > upper_bound[col] else x)\n",
        "        data[col] = data[col].apply(lambda x: lower_bound[col] if x < lower_bound[col] else x)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Replace the outliers in each column with the lower or upper bound\n",
        "X_train = replace_outliers(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XUKkjtzyidws"
      },
      "outputs": [],
      "source": [
        "# Data scaling\n",
        "scaler = MinMaxScaler()  # define scaler\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)  # fit scaler on train set\n",
        "X_val = scaler.transform(X_val)  # apply fitted scaler on validation set\n",
        "X_test = scaler.transform(X_test)  # apply fitted scaler on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7_ogeiUgidws"
      },
      "outputs": [],
      "source": [
        "# Remove all constant (zero-variance) features\n",
        "X_train = pd.DataFrame(X_train)\n",
        "zero_var_filter = VarianceThreshold(threshold=0)\n",
        "\n",
        "# Fit on train data\n",
        "zero_var_filter.fit(X_train)\n",
        "zero_var_columns = [column for column in X_train.columns if column not in X_train.columns[zero_var_filter.get_support()]]\n",
        "\n",
        "# Apply on train, validation and test data\n",
        "X_train = zero_var_filter.transform(X_train)\n",
        "X_val = zero_var_filter.transform(X_val)\n",
        "X_test = zero_var_filter.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CQMg1damidws",
        "outputId": "17cc2c8f-6648-4941-afcf-02f35de49ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 228 from 455 features.\n"
          ]
        }
      ],
      "source": [
        "# ----- Feature selection -----\n",
        "lasso_selector = SelectFromModel(estimator=Lasso(alpha=10**(-10), max_iter=1000), threshold='median')\n",
        "lasso_selector.fit(X_train, y_train)\n",
        "lasso_list = [column for column in pd.DataFrame(X_train).columns[lasso_selector.get_support()]]\n",
        "n_original = X_train.shape[1]\n",
        "\n",
        "X_train = lasso_selector.transform(X_train)\n",
        "n_selected = X_train.shape[1]\n",
        "print(f\"Selected {n_selected} from {n_original} features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4u7OWnl9idwt",
        "outputId": "1a8d169c-cbeb-423b-ede9-dc241ac9fe3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 50 features to be used for classification.\n"
          ]
        }
      ],
      "source": [
        "# ----- Feature extraction -----\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 95% variance\n",
        "pca = PCA(n_components = 0.95)\n",
        "pca.fit(X_train)\n",
        "X_train = pca.transform(X_train)\n",
        "\n",
        "print(f\"Selected {X_train.shape[1]} features to be used for classification.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mfF07SND4TZ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}